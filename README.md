# ElasticSearch Cluster on AKS

**Tested on Azure, This deployment will work in any other cloud but have to change the configurations for storageclass and loadbalancer configurations**
**This deployment using ES version 6.2.4, will not support version 7.x.x**

[kudos](https://medium.com/faun/https-medium-com-thakur-vaibhav23-ha-es-k8s-7e655c1b7b61)

[help](https://www.datadoghq.com/blog/elasticsearch-game-day/)

# Current software

* Alpine Linux 3.8
* OpenJDK JRE 8u171
* Elasticsearch 6.2.4

**Note:** `x-pack-ml` module is forcibly disabled as it's not supported on Alpine Linux.



# Authentication for ElasticSearch and Kibana

This kubernetes deployment by default enabling the authetication for elasticsearch and kibana. If you don't need authetication you can simply remove the `AUTH_CONFIG` environment variable.

These are the default Credentials for the deployment until and unless you remove the `AUTH_CONFIG` environment variable.

The Usernames are hardcoded but you can override this credentials (only passwords) by updating these environment variables from Configmap (I already gave it as an example in the deployment part) or from secrets in kubernetes


| User Name | Variable Name | Default Value | Description |
|---------------|---------------|---------------|---------------|
| kibanAdmin | KIBANA_ADMIN_PASSWORD | Admin@Kibana | Have full permission on the kibana dashboard |
| kibanaUser | KIBANA_RO_PASSWORD | Ro@Kibana | Have readonly access on kibana dashboard |
| LogAdmin | PUSHLOG_PASSWORD | Push2ES | Password for elasticsearch authetication (used by log shippers) |


**Note : For Kibana Auth Configiuration while deploying, you have to configure User : kibanAdmin and Pass : KIBANA_ADMIN_PASSWORD**


# Kubernetes Deployment 

kubernetes issue : [Not support new ES version](https://hub.helm.sh/charts/stable/elasticsearch)

**Run these commands from `Kubernetes-Deployment` Folder**

## Create Namespace for Elasticsearch Deployment
```
kubectl apply -f es-namespace.yaml
```

## Deploy master nodes
```
kubectl apply -f ElasticSearch/es-master-deployment.yaml
kubectl apply -f ElasticSearch/es-master-service.yaml
kubectl -n elasticsearch get pods
```

## Deploy data nodes
```
kubectl apply -f ElasticSearch/es-data-storageclass.yaml
kubectl get storageclass
kubectl apply -f ElasticSearch/es-data-deployment.yaml
kubectl apply -f ElasticSearch/es-data-service.yaml
kubectl -n elasticsearch get pods
```

## Deploy client nodes
```
kubectl apply -f ElasticSearch/es-client-deployment.yaml
kubectl apply -f ElasticSearch/es-client-service.yaml
kubectl -n elasticsearch get pods
```

ElasticSearch can access on this url : `http://<<es_client_loadbalancer_IPAddress>>:9200`


## Deploy Elastic HQ (Monitoring tool)

**Update the ConfigMap : `es-hq-configmap.yaml` with the basic-auth username and password**

```
kubectl apply -f ES-HQ/es-hq-configmap.yaml
kubectl apply -f ES-HQ/es-hq-deployment.yaml
kubectl apply -f ES-HQ/es-hq-service.yaml
kubectl -n elasticsearch get pods
```

Elastic HQ can access on this url : `http://<<es_HQ_loadbalancer_IPAddress>>`

## Deploy Kibana Server
```
kubectl -n elasticsearch create configmap kibana-config-cm --from-file=kibana/kibana.yml
kubectl apply -f kibana/kibana-deployment.yaml
kubectl apply -f kibana/kibana-service.yaml
kubectl -n elasticsearch get pods
```

kibana can access on this url : `http://<<kibana_loadbalancer_IPAddress>>/app/kibana`


## Deploy Elastic APM Server
```
kubectl -n elasticsearch create configmap apm-server-cm --from-file=ES-APM/apm-server.yml
kubectl apply -f ES-APM/es-apm-deployment.yaml
kubectl apply -f ES-APM/es-apm-service.yaml
kubectl -n elasticsearch get pods
```

Elastic APM can access on this url : `http://<<es_APM_loadbalancer_IPAddress>>`

## Check the services, pods, pvc and pv
```
kubectl -n elasticsearch get pods
kubectl -n elasticsearch get services
kubectl -n elasticsearch get pvc
kubectl get pv
```


## Scaling Considerations

### client nodes

We can deploy autoscalers for our client nodes depending upon our CPU thresholds. A sample HPA for client node might look something like this:

**NOTE : you have to install and configure matrics in your cluster**

```
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: es-client
  namespace: elasticsearch
spec:
  maxReplicas: 5
  minReplicas: 2
  scaleTargetRef:
    apiVersion: extensions/v1beta1
    kind: Deployment
    name: es-client
  targetCPUUtilizationPercentage: 80
```

Whenever the autoscaler will kick in, we can watch the new client-node pods being added to the cluster, by observing the logs of any of the master-node pods.

### data nodes

In case of Data-Node Pods all we have to do it increase the number of replicas using the K8 Dashboard or GKE console. The newly created data node will be automatically added to the cluster and start replicating data from other nodes.

### master nodes

Master-Node Pods do not require autoscaling as they only store cluster-state information but in case you want to add more data nodes make sure there are no even number of master nodes in the cluster also the environment variable NUMBER_OF_MASTERS is updated accordingly.



## PVC Resizing Configuration

[here](https://kubernetes.io/blog/2018/07/12/resizing-persistent-volumes-using-kubernetes/)

1. Delete all the pods which are using pvc (delete the StatefulSet)
2. edit all pvc with as follows

`kubectl -n elasticsearch edit  pvc storage-es-data-0`


```
uodate the storage entry.
```

3. redeploy the `StatefulSet`



## The LOG Battle: Logstash and Fluentd

* source data processing pipeline
  * Logstash
  * Fluentd
* lightweight shippers
  * Filebeat
  * Fluentbit

[here](https://medium.com/tensult/the-log-battle-logstash-and-fluentd-c65f2f7c24b4)

### filebeat

[installation 1](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-installation.html)

[installation 2](https://crunchify.com/setup-guide-install-configure-filebeat/)

[configuration](https://www.elastic.co/guide/en/beats/filebeat/5.1/filebeat-configuration-details.html)


## Run

### Attention

* In order for `bootstrap.mlockall` to work, `ulimit` must be allowed to run in the container. Run with `--privileged` to enable this.
* [Multicast discovery is no longer built-in](https://www.elastic.co/guide/en/elasticsearch/reference/2.3/breaking_20_removed_features.html#_multicast_discovery_is_now_a_plugin)

Ready to use node for cluster `elasticsearch-default`:
```
docker run --name elasticsearch \
	--detach \
	--privileged \
	--volume /path/to/data_folder:/data \
        akhilrajmailbox/elasticsearch:elasticsearch-6.2.4
```

Ready to use node for cluster `myclustername`:
```
docker run --name elasticsearch \
	--detach \
	--privileged \
	--volume /path/to/data_folder:/data \
	-e CLUSTER_NAME=myclustername \
        akhilrajmailbox/elasticsearch:elasticsearch-6.2.4
```

Ready to use node for cluster `elasticsearch-default`, with 8GB heap allocated to Elasticsearch:
```
docker run --name elasticsearch \
	--detach \
	--privileged \
	--volume /path/to/data_folder:/data \
	-e ES_JAVA_OPTS="-Xms8g -Xmx8g" \
        akhilrajmailbox/elasticsearch:elasticsearch-6.2.4
```

Ready to use node with plugins (x-pack and repository-gcs) pre installed. Already installed plugins are ignored:
```
docker run --name elasticsearch \
	--detach \
	--privileged \
	--volume /path/to/data_folder:/data \
	-e ES_JAVA_OPTS="-Xms8g -Xmx8g" \
	-e ES_PLUGINS_INSTALL="repository-gcs,x-pack" \
        akhilrajmailbox/elasticsearch:elasticsearch-6.2.4
```

**Master-only** node for cluster `elasticsearch-default`:
```
docker run --name elasticsearch \
	--detach \
	--privileged \
	--volume /path/to/data_folder:/data \
	-e NODE_DATA=false \
	-e HTTP_ENABLE=false \
        akhilrajmailbox/elasticsearch:elasticsearch-6.2.4
```

**Data-only** node for cluster `elasticsearch-default`:
```
docker run --name elasticsearch \
	--detach --volume /path/to/data_folder:/data \
	--privileged \
	-e NODE_MASTER=false \
	-e HTTP_ENABLE=false \
        akhilrajmailbox/elasticsearch:elasticsearch-6.2.4
```

**Data-only** node for cluster `elasticsearch-default` with shard allocation awareness:
```
docker run --name elasticsearch \
	--detach --volume /path/to/data_folder:/data \
        --volume /etc/hostname:/dockerhost \
	--privileged \
	-e NODE_MASTER=false \
	-e HTTP_ENABLE=false \
    -e SHARD_ALLOCATION_AWARENESS=dockerhostname \
    -e SHARD_ALLOCATION_AWARENESS_ATTR="/dockerhost" \
        akhilrajmailbox/elasticsearch:elasticsearch-6.2.4
```

**Client-only** node for cluster `elasticsearch-default`:
```
docker run --name elasticsearch \
	--detach \
	--privileged \
	--volume /path/to/data_folder:/data \
	-e NODE_MASTER=false \
	-e NODE_DATA=false \
        akhilrajmailbox/elasticsearch:elasticsearch-6.2.4
```
I also make available special images and instructions for [AWS EC2](https://github.com/pires/docker-elasticsearch-aws) and [Kubernetes](https://github.com/pires/docker-elasticsearch-kubernetes).

### Environment variables

This image can be configured by means of environment variables, that one can set on a `Deployment`.

* [CLUSTER_NAME](https://www.elastic.co/guide/en/elasticsearch/reference/current/important-settings.html#cluster.name)
* [NODE_NAME](https://www.elastic.co/guide/en/elasticsearch/reference/current/important-settings.html#node.name)
* [NODE_MASTER](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html#master-node)
* [NODE_DATA](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html#data-node)
* [NETWORK_HOST](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-network.html#network-interface-values)
* [HTTP_ENABLE](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-http.html#_settings_2)
* [HTTP_CORS_ENABLE](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-http.html#_settings_2)
* [HTTP_CORS_ALLOW_ORIGIN](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-http.html#_settings_2)
* [NUMBER_OF_MASTERS](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-zen.html#master-election)
* [MAX_LOCAL_STORAGE_NODES](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html#max-local-storage-nodes)
* [ES_JAVA_OPTS](https://www.elastic.co/guide/en/elasticsearch/reference/current/heap-size.html)
* [ES_PLUGINS_INSTALL](https://www.elastic.co/guide/en/elasticsearch/plugins/current/installation.html) - comma separated list of Elasticsearch plugins to be installed. Example: `ES_PLUGINS_INSTALL="repository-gcs,x-pack"`
* [SHARD_ALLOCATION_AWARENESS](https://www.elastic.co/guide/en/elasticsearch/reference/current/allocation-awareness.html#CO287-1)
* [SHARD_ALLOCATION_AWARENESS_ATTR](https://www.elastic.co/guide/en/elasticsearch/reference/current/allocation-awareness.html#CO287-1)
* [MEMORY_LOCK](https://www.elastic.co/guide/en/elasticsearch/reference/current/important-settings.html#bootstrap.memory_lock) - memory locking control - enable to prevent swap (default = `true`) .
* [REPO_LOCATIONS](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html#_shared_file_system_repository) - list of registered repository locations. For example `"/backup"` (default = `[]`). The value of REPO_LOCATIONS is automatically wrapped within an `[]` and therefore should not be included in the variable declaration. To specify multiple repository locations simply specify a comma separated string for example `"/backup", "/backup2"`.
* [PROCESSORS](https://github.com/elastic/elasticsearch-definitive-guide/pull/679/files) - allow elasticsearch to optimize for the actual number of available cpus (must be an integer - default = 1)

### Backup
Mount a shared folder (for example via NFS) to `/backup` and make sure the `elasticsearch` user
has write access. Then, set the `REPO_LOCATIONS` environment variable to `"/backup"` and create
a backup repository:

`backup_repository.json`:
```
{
  "type": "fs",
  "settings": {
    "location": "/backup",
    "compress": true
  }
}
```

```bash
curl -XPOST http://<container_ip>:9200/_snapshot/nas_repository -d @backup_repository.json`
```

Now, you can take snapshots using:
```bash
curl -f -XPUT "http://<container_ip>:9200/_snapshot/nas_repository/snapshot_`date --utc +%Y_%m_%dt%H_%M`?wait_for_completion=true"
```
